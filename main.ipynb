{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmsFABwClrsS"
   },
   "source": [
    "# Encoder-decoder architecture\n",
    "\n",
    "Encoder-decoder architectures are about converting anything to anything, including\n",
    " * Machine translation and spoken dialogue systems\n",
    " * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)\n",
    " * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n",
    " * Grapheme2phoneme - convert words to transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vocab import Vocab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_models import BasicModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4N9AD2dlrsU"
   },
   "source": [
    "# Our task: machine translation\n",
    "\n",
    "We gonna try our encoder-decoder models on russian to english machine translation problem. More specifically, we'll translate hotel and hostel descriptions.\n",
    "\n",
    "* Data will be tokenized with WordPunctTokenizer.\n",
    "\n",
    "* Our data lines contain unique rare words. If we operate on a word level, we will have to deal with large vocabulary size. If instead we use character-level models, it would take lots of iterations to process a sequence. This time we're gonna pick something inbetween.\n",
    "\n",
    "* One popular approach is called [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) aka __BPE__. The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "CfvojjHQlrsU",
    "outputId": "aa95011d-c2f7-4ca3-aec1-b246b9c3f886",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch>=1.3.0\n",
    "!pip3 install subword-nmt &> log\n",
    "# !wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py -O vocab.py\n",
    "# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g9kP0SdxlrsY"
   },
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    return ' '.join(nltk_tokenizer.tokenize(x.lower()))\n",
    "\n",
    "\n",
    "nltk_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# split and tokenize the data\n",
    "with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:\n",
    "    for line in open('data.txt'):\n",
    "        src_line, dst_line = line.strip().split('\\t')\n",
    "        f_src.write(tokenize(src_line) + '\\n')\n",
    "        f_dst.write(tokenize(dst_line) + '\\n')\n",
    "\n",
    "# build and apply bpe vocs\n",
    "bpe = {}\n",
    "for lang in ['en', 'ru']:\n",
    "    learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
    "    bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
    "    \n",
    "    with open('train.bpe.' + lang, 'w') as f_out:\n",
    "        for line in open('train.' + lang):\n",
    "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UPW3sV8lrsb"
   },
   "source": [
    "## Building vocabularies\n",
    "\n",
    "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "8PskgBSxlrsd",
    "outputId": "dfb78818-90cf-4138-c1f6-3f492ba06a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 50001\n",
      "\n",
      "Inp: для гостей сервируется завтрак . кроме того , гости могут воспользоваться общей кухней .\n",
      "Out: breakfast is provided and common kitchen facilities are featured .\n",
      "\n",
      "Inp: в числе удобств всех номеров — мини - бар и телевизор со спутниковыми каналами , а в некоторых номерах есть гостиная зона и кондиционер .\n",
      "Out: each comes with a minibar and satellite tv . some offer a seating area and air conditioning .\n",
      "\n",
      "Inp: фитнес - центр гостиницы располагает 5 тренаж@@ ерами для фит@@ нес@@ а , а также мест@@ ами для игр и оборудованием для детей .\n",
      "Out: the fitness centre of the hotel provides 5 types of fitness de@@ vi@@ ces and it offers games and special children - friendly equipment .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
    "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
    "\n",
    "assert len(data_inp) == len(data_out), 'Number of inp & out sequences must be the same'\n",
    "\n",
    "print('Number of sequences: %s' % len(data_inp), end='\\n\\n')\n",
    "\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=0.05,\n",
    "                                                          random_state=0)\n",
    "for i in range(3):\n",
    "    print('Inp:', train_inp[i])\n",
    "    print('Out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vipg4O61lrsg"
   },
   "outputs": [],
   "source": [
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "cwOoHfuhlrsi",
    "outputId": "3a94bd2b-f34c-49ce-ad13-1ee1215d76c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: ['имеется балкон .', 'гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .']\n",
      "\n",
      "Words to ids (0 = BOS, 1 = EOS):\n",
      " tensor([[   0, 3487, 1870,   29,    1,    1,    1,    1],\n",
      "        [   0, 2690, 2944, 1108,   29,    1,    1,    1],\n",
      "        [   0, 2923, 1834, 8035,   59, 3800,   29,    1],\n",
      "        [   0, 6028, 2084,   29,    1,    1,    1,    1],\n",
      "        [   0, 4928, 1870,   29,    1,    1,    1,    1]])\n",
      "\n",
      "Back to words: ['имеется балкон .', 'гостевой дом r .', 'до афин — 20 км .', 'работает боулинг .', 'оборудован балкон .']\n"
     ]
    }
   ],
   "source": [
    "# Here's how you cast lines into ids and backwards\n",
    "batch_lines = sorted(train_inp, key=len)[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print('Lines: %s' % batch_lines, end='\\n\\n')\n",
    "print('Words to ids (0 = BOS, 1 = EOS):\\n %s' % batch_ids, end='\\n\\n') # BOS = Begin Of Sentence\n",
    "print('Back to words: %s' % batch_lines_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSYu-MkElrsk"
   },
   "source": [
    "## Draw source and translation length distributions to estimate the scope of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "TLLl9cSNlrsl",
    "outputId": "fa1ad0ae-c0b1-4199-f2c6-5dd16d8aa303"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHWgx34flrsn"
   },
   "source": [
    "## Encoder-decoder model\n",
    "\n",
    "The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pd_rDRm9lrso",
    "outputId": "65302dbe-4309-412d-f460-3c4f87054236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OzdVCCwOSEhv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations without training:\n",
      "list isa isa eno abi inese curtains piano relaxing chelsea lockers ronomical galeão arena spain dresden tim@@ ig@@ ardo designs т ly@@ ho@@ pak@@ concert\n",
      "sand rela@@ expl@@ lounger beck discounts few los zen ataturk pur@@ not@@ speed ld ě queen@@ fru@@ slo@@ motorbikes cakes clock utt@@ 16@@ amen floor\n"
     ]
    }
   ],
   "source": [
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "\n",
    "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
    "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
    "\n",
    "h0 = model.encode(dummy_inp_tokens)\n",
    "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
    "\n",
    "assert isinstance(h1, list) and len(h1) == len(h0)\n",
    "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
    "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
    "\n",
    "logits_seq = model.decode(h0, dummy_out_tokens)\n",
    "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
    "\n",
    "# full forward\n",
    "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
    "assert logits_seq2.shape == logits_seq.shape\n",
    "\n",
    "dummy_translations, dummy_states = model.translate_lines(train_inp[:2], device, max_len=25)\n",
    "translations = '\\n'.join([line for line in dummy_translations])\n",
    "print('Translations without training:\\n%s' % translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wuv1-aVlrs0"
   },
   "source": [
    "## Training loss\n",
    "\n",
    "Our training objective is almost the same as it was for neural language models:\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
    "\n",
    "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c8XPV8sWlrs5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(7.5420, device='cuda:3', grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(model, inp, out, **flags):\n",
    "    \"\"\"\n",
    "    Compute loss (float32 scalar) as in the formula above\n",
    "    :param inp: input tokens matrix, int32[batch, time]\n",
    "    :param out: reference tokens matrix, int32[batch, time]\n",
    "    \n",
    "    In order to pass the tests, your function should\n",
    "    * include loss at first EOS but not the subsequent ones\n",
    "    * divide sum of losses by a sum of input lengths (use voc.compute_mask)\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = model.out_voc.compute_mask(out) # [batch_size, out_len]\n",
    "    targets_one_hot = F.one_hot(out, len(model.out_voc)).to(torch.float32)\n",
    "    \n",
    "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
    "    logits_seq = model(inp, out)\n",
    "    \n",
    "    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]\n",
    "    logprobs_seq = torch.log_softmax(logits_seq, dim=-1)\n",
    "    \n",
    "    # log-probabilities of correct outputs, [batch_size, out_len]\n",
    "    logp_out = (logprobs_seq * targets_one_hot).sum(dim=-1)\n",
    "        \n",
    "    # average cross-entropy over tokens where mask == True\n",
    "    return -logp_out[mask].mean()\n",
    "\n",
    "\n",
    "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
    "print('Loss: %s' % dummy_loss)\n",
    "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1), 'Sorry for your loss'\n",
    "\n",
    "# test autograd\n",
    "dummy_loss.backward()\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.grad is not None and abs(param.grad.max()) != 0, 'Param %s received no gradients' % name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpbaBpW7lrs-"
   },
   "source": [
    "## Evaluation: BLEU\n",
    "\n",
    "Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.\n",
    "\n",
    "While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Gb1-PhKIlrs-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025311331957273363"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_bleu(model, inp_lines, out_lines, bpe_sep='@@ ', **flags):\n",
    "    \"\"\"\n",
    "    Estimates corpora-level BLEU score of model's translations given inp and reference out\n",
    "    Note: if you're serious about reporting your results, use https://pypi.org/project/sacrebleu\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        translations, _ = model.translate_lines(inp_lines, device, **flags)\n",
    "        translations = [line.replace(bpe_sep, '') for line in translations]\n",
    "        actual = [line.replace(bpe_sep, '') for line in out_lines]\n",
    "        return corpus_bleu(\n",
    "            [[ref.split()] for ref in actual],\n",
    "            [trans.split() for trans in translations],\n",
    "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
    "            ) * 100\n",
    "\n",
    "\n",
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQDhGwg4lrtC"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yfwIaixHlrtI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, n_iters=15000, batch_size=64, lr=1e-3):\n",
    "    metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        step = len(metrics['train_loss']) + 1\n",
    "        batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "        batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "        batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "        loss_t.backward()\n",
    "        opt.step()\n",
    "\n",
    "        metrics['train_loss'].append((step, loss_t.item()))\n",
    "        if step % 200 == 0:\n",
    "            metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                plt.subplot(1, len(metrics), i + 1)\n",
    "                plt.title(name)\n",
    "                plt.plot(*zip(*history))\n",
    "                plt.grid()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "LlDT6eDUlrtL",
    "outputId": "d0299ee4-2afb-466b-b767-98c09d6ee5e3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss=1.970\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-54989e7714a1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, n_iters, batch_size, lr)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "basic_model = BasicModel(inp_voc, out_voc).to(device)\n",
    "%time train_model(basic_model, batch_size=128, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(basic_model.state_dict(), \"15.28blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KyaHOpealrtS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: в распоряжении гостей караоке .\n",
      "Out: guests can enjoy a meal at the property .\n",
      "\n",
      "Inp: рядом со зданием предоставляется бесплатная общественная парковка .\n",
      "Out: free public parking is available on site .\n",
      "\n",
      "Inp: из отеля открывается вид на окружающие горы и пустыню .\n",
      "Out: the property boasts views of the surrounding mountains and mountains .\n",
      "\n",
      "Inp: гости отеля могут посещать хорошо оборудованный фитнес - центр .\n",
      "Out: guests can enjoy a workout in the fitness centre , or make use of the property .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::700], basic_model.translate_lines(dev_inp[::700], device)[0]):\n",
    "    print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.401409112583888 1300\n",
      "58.59894259713782 1302\n",
      "63.16558375510932 1314\n",
      "88.94446168073998 1399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-456ef1c54823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdev_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdev_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdev_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdev_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f12806b379f4>\u001b[0m in \u001b[0;36mcompute_bleu\u001b[0;34m(model, inp_lines, out_lines, bpe_sep, **flags)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NMT/my_models.py\u001b[0m in \u001b[0;36mtranslate_lines\u001b[0;34m(self, inp_lines, device, beam_size, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mout_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mout_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_inference_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NMT/my_models.py\u001b[0m in \u001b[0;36mdecode_inference_beam_search\u001b[0;34m(self, initial_state, beam_size, max_len, **flags)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mprev_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mcur_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mstates_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NMT/my_models.py\u001b[0m in \u001b[0;36mdecode_step\u001b[0;34m(self, prev_state, prev_tokens, **flags)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run my_models.py\n",
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "model.load_state_dict(torch.load(\"15.28blue\"))\n",
    "\n",
    "# basic_model.translate_lines([dev_inp[0]], device, 2)\n",
    "\n",
    "# print(compute_bleu(model, dev_inp, dev_out))\n",
    "# print(compute_bleu(model, dev_inp, dev_out, beam_size=2))\n",
    "\n",
    "\n",
    "mx = -1\n",
    "a = []\n",
    "b = []\n",
    "for i in range(1300, 1600):\n",
    "    x = compute_bleu(model, [dev_inp[i]], [dev_out[i]])\n",
    "    y = compute_bleu(model, [dev_inp[i]], [dev_out[i]], beam_size=4)\n",
    "    a.append(x)\n",
    "    b.append(y)\n",
    "    if x - y > mx:\n",
    "        mx = x - y\n",
    "        print(mx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.833363909115658 10.004011939039914\n",
      "22.690589525503256 9.02660651368419\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(a), np.percentile(a, 20))\n",
    "print(np.mean(b), np.percentile(b, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the property offers free parking .'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1314\n",
    "x = compute_bleu(model, [dev_inp[i]], [dev_out[i]])\n",
    "y = compute_bleu(model, [dev_inp[i]], [dev_out[i]], beam_size=2)\n",
    "x, y\n",
    "dev_out[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dev_inp[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: на полностью оборудованной кухне установлены посудомоечная машина и микроволновая печь .\n",
      "Out: there is a full kitchen with a dishwasher and a microwave .\n",
      "Out: there is fully equipped with dishwasher and dishwasher .\n"
     ]
    }
   ],
   "source": [
    "%run my_models.py\n",
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "model.load_state_dict(torch.load(\"15.28blue\"))\n",
    "\n",
    "inp_line = dev_inp[1399]\n",
    "trans1 = model.translate_lines([inp_line], device)[0][0]\n",
    "trans2 = model.translate_lines([inp_line], device, 4, max_len=100)[0][0]\n",
    "\n",
    "print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "print('Out: %s' % trans1.replace('@@ ', ''))\n",
    "print('Out: %s' % trans2.replace('@@ ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d019f73f18da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# compute_bleu(basic_model, dev_inp[::10], dev_out[::10], beam_size=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-f12806b379f4>\u001b[0m in \u001b[0;36mcompute_bleu\u001b[0;34m(model, inp_lines, out_lines, bpe_sep, **flags)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NMT/my_models.py\u001b[0m in \u001b[0;36mtranslate_lines\u001b[0;34m(self, inp_lines, device, beam_size, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mout_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_inference_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NMT/vocab.py\u001b[0m in \u001b[0;36mto_lines\u001b[0;34m(self, matrix, crop)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mline_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0mline_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mline_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run my_models.py\n",
    "basic_model = BasicModel(inp_voc, out_voc).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: в распоряжении гостей караоке .\n",
      "Out: you will find a meal at the property .\n",
      "\n",
      "Inp: рядом со зданием предоставляется бесплатная общественная парковка .\n",
      "Out: free public parking is available .\n",
      "\n",
      "Inp: из отеля открывается вид на окружающие горы и пустыню .\n",
      "Out: it has a private bathrooms and a private bathroom .\n",
      "\n",
      "Inp: гости отеля могут посещать хорошо оборудованный фитнес - центр .\n",
      "Out: the hotel also has a fitness centre with a fitness centre .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::700], basic_model.translate_lines(dev_inp[::700], device, 2)[0]):\n",
    "    print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypos = [[] for _ in range(3)]\n",
    "hypos[0].append(1)\n",
    "if not hypos[0]:\n",
    "    print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[2001:2004], basic_model.translate_lines(dev_inp[2001:2004], device)[0]):\n",
    "    print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2003\n",
    "compute_bleu(basic_model, [dev_inp[idx]], [dev_out[idx]]), compute_bleu(basic_model, [dev_inp[idx]], [dev_out[idx]], beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.29238928349384"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(model, dev_inp[::10], dev_out[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.29238928349384"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(model, dev_inp[::10], dev_out[::10], beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2], [0, 2], [3, 228], [2, -4]]\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_range = range(1, 11)\n",
    "blue_scores = []\n",
    "\n",
    "for beam_size in beam_range:\n",
    "    %time blue = compute_bleu(basic_model, dev_inp, dev_out, beam_size=beam_size)\n",
    "    blue_scores.append(blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(beam_range, blue_scores)\n",
    "plt.title('Beam search results')\n",
    "plt.xlabel('beam size')\n",
    "plt.ylabel('blue score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time compute_bleu(basic_model, dev_inp, dev_out, beam_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time compute_bleu(basic_model, dev_inp, dev_out, beam_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Encoder-decoder model gets bleu 17.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edk_oVg0lrtW"
   },
   "source": [
    "* We want you to improve over the basic model by implementing a simple attention mechanism\n",
    "\n",
    "* This is gonna be a two-parter: building the __attention layer__ and using it for an __attentive seq2seq model__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz9aROAIlrtX"
   },
   "source": [
    "### Attention layer\n",
    "\n",
    "Here you will have to implement a layer that computes a simple additive attention:\n",
    "\n",
    "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
    "\n",
    "* Compute logits with a 2-layer neural network\n",
    "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
    "* Get probabilities from logits, \n",
    "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
    "\n",
    "* Add up encoder states with probabilities to get __attention response__\n",
    "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
    "\n",
    "You can learn more about attention layers in the lecture slides or [from this post](https://distill.pub/2016/augmented-rnns/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IalfpdAelrtb"
   },
   "source": [
    "## Seq2seq model with attention\n",
    "\n",
    "You can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase:\n",
    "![img](https://i.imgur.com/6fKHlHb.png)\n",
    "_image from distill.pub [article](https://distill.pub/2016/augmented-rnns/)_\n",
    "\n",
    "On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attention layer.\n",
    "\n",
    "The key implementation detail here is __model state__. Put simply, you can add any tensor into the list of `encode` outputs. You will then have access to them at each `decode` step. This may include:\n",
    "* Last RNN hidden states (as in basic model)\n",
    "* The whole sequence of encoder outputs (to attend to) and mask\n",
    "* Attention probabilities (to visualize)\n",
    "\n",
    "_There are alternative ways to wire attention into network and different kinds of attention. For example [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCKPB5JmcE6j"
   },
   "outputs": [],
   "source": [
    "from my_models import AttentiveModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryZCOTEslrtf"
   },
   "source": [
    "### Training attentive model\n",
    "\n",
    "Please reuse the infrastructure you've built for the regular model. I hope you didn't hard-code anything :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "-YMHPgZxcFaQ",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss=1.339\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-54989e7714a1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, n_iters, batch_size, lr)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attentive_model = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "%time train_model(attentive_model, n_iters=6000, lr=1e-3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentive_model.load_state_dict(torch.load(\"att\"))\n",
    "# torch.save(attentive_model.state_dict(), \"att\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-5Pwl2yZSEib"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: в распоряжении гостей караоке .\n",
      "Out: guests can also enjoy the on - site .\n",
      "\n",
      "Inp: в 1 , 4 км находится концертная площадка arena di verona .\n",
      "Out: the property is 1 . 4 km from the property .\n",
      "\n",
      "Inp: к вашим услугам ресторан , где подают блюда из различных уголков мира , а также круглосуточное обслуживание номеров .\n",
      "Out: it offers a restaurant , with a selection of chinese and breakfast .\n",
      "\n",
      "Inp: в ванных комнатах установлен душ .\n",
      "Out: the bathrooms are fitted with a shower .\n",
      "\n",
      "Inp: расстояние до аэропорта биллунна составляет 55 км .\n",
      "Out: this holiday home is 55 km from billund airport .\n",
      "\n",
      "Inp: в номерах отеля типа « постель и завтрак » doonshean view есть душ , телевизор с плоским экраном и каналами сети freeview , фен , будильник и принадлежности для чая / кофе .\n",
      "Out: rooms at the bed & breakfast have parquet floors and flat - screen cable tv , a hairdryer , tea / coffee making facilities .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], attentive_model.translate_lines(dev_inp[::500], device)[0]):\n",
    "    print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bd22b15c5386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattentive_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattentive_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattentive_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f12806b379f4>\u001b[0m in \u001b[0;36mcompute_bleu\u001b[0;34m(model, inp_lines, out_lines, bpe_sep, **flags)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_sep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NMT/my_models.py\u001b[0m in \u001b[0;36mtranslate_lines\u001b[0;34m(self, inp_lines, device, beam_size, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mout_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_inference_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_voc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NMT/vocab.py\u001b[0m in \u001b[0;36mto_lines\u001b[0;34m(self, matrix, crop)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0mline_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_ix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%run my_models.py\n",
    "\n",
    "attentive_model = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "# attentive_model.load_state_dict(torch.load(\"att\"))\n",
    "\n",
    "x = 1000\n",
    "print(compute_bleu(attentive_model, dev_inp[::x], dev_out[::x]))\n",
    "print(compute_bleu(attentive_model, dev_inp[::x], dev_out[::x], beam_size=2))\n",
    "print(compute_bleu(attentive_model, dev_inp[::x], dev_out[::x], beam_size=3))\n",
    "\n",
    "\n",
    "# mx = -1\n",
    "# a = []\n",
    "# b = []\n",
    "# for i in range(1360, 1400):\n",
    "#     x = compute_bleu(attentive_model, [dev_inp[i]], [dev_out[i]])\n",
    "#     y = compute_bleu(attentive_model, [dev_inp[i]], [dev_out[i]], beam_size=4)\n",
    "#     a.append(x)\n",
    "#     b.append(y)\n",
    "#     if x - y > 50:\n",
    "#         mx = x - y\n",
    "#         print(mx, i)\n",
    "\n",
    "# 25.200817841164223\n",
    "# 26.78082511990569\n",
    "# 27.001900732799466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 2, 3]), 4], [array([10,  2,  3]), 4])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "a = [np.array([1, 2, 3]), 4]\n",
    "b = deepcopy(a)\n",
    "b[0][0] = 10\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: расстояние до аэропорта париж – орли составляет 17 км .\n",
      "Out: the nearest airport is paris - orly airport , 17 km from the property .\n",
      "Out: the nearest airport is located 17 km away .\n",
      "\n",
      "Inp: расстояние до аэропорта биллунн составляет 90 км .\n",
      "Out: this holiday home is 90 km from billund airport .\n",
      "Out: this holiday home is 90 airport is 90 km away .\n",
      "\n",
      "Inp: на полностью оборудованной кухне установлены посудомоечная машина и микроволновая печь .\n",
      "Out: there is a full kitchen with a dishwasher and a microwave .\n",
      "Out: there is fully equipped kitchen is equipped with a dishwasher and a microwave .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run my_models.py\n",
    "attentive_model = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "attentive_model.load_state_dict(torch.load(\"att\"))\n",
    "\n",
    "for i in [1364, 1386, 1399]:\n",
    "    inp_line = dev_inp[i]\n",
    "    trans1 = attentive_model.translate_lines([inp_line], device)[0][0]\n",
    "    trans2 = attentive_model.translate_lines([inp_line], device, 5, max_len=100)[0][0]\n",
    "\n",
    "    print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans1.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans2.replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exp(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-bb9f0c66fa9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: exp(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "torch.exp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "228\n",
      "228\n",
      "228\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    i = 228\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: в распоряжении гостей караоке .\n",
      "Out: guests can also enjoy the on - site .\n",
      "\n",
      "Inp: гостевой дом hosteria posada drake находится в 800 метрах от автобусного вокзала пуэрто - сан - хулиан .\n",
      "Out: the guest house is located 800 metres from the bus stop in rio de janeiro .\n",
      "\n",
      "Inp: расстояние до международного аэропорта куала - лумпура составляет 88 км .\n",
      "Out: the nearest airport is dalian d ' assisi airport , 88 km from the property .\n",
      "\n",
      "Inp: апартаменты ariosa с бесплатным wi - fi находятся в городе мтвапа .\n",
      "Out: apartment home offers pet - friendly accommodation in saint petersburg .\n",
      "\n",
      "Inp: в 500 метрах находится 9 - луночное гольф - поле ehrwald .\n",
      "Out: the sierra de - moneken golf course is 500 metres away .\n",
      "\n",
      "Inp: к вашим услугам ресторан , где подают блюда из различных уголков мира , а также круглосуточное обслуживание номеров .\n",
      "Out: it offers a restaurant , with a selection of chinese and breakfast .\n",
      "\n",
      "Inp: в ресторане über den wolken на 36 этаже отеля можно заказать кофе и пирожные , а из окон открывается вид на залив любек .\n",
      "Out: in the morning ’ s restaurant , guests can enjoy panoramic views of the mediterranean cuisine , while the grill is served in the spacious conservatory .\n",
      "\n",
      "Inp: из отеля открывается вид на окружающие горы и пустыню .\n",
      "Out: the hotel overlooks the surrounding mountains and views of the old town .\n",
      "\n",
      "Inp: прогулка до ближайшей остановки вапоретто giardini - biennale займет 7 минут .\n",
      "Out: the property is within a 7 - minute walk of the san francisco ’ s famous cambridge university of san francisco , and the nearest golf course is 7 minutes ’ drive .\n",
      "\n",
      "Inp: апартаменты sunny view north находятся всего в 10 минутах ходьбы от пляжа .\n",
      "Out: just a 10 - minute walk from the beach , apartments on the sea are just a 10 - minute walk from the beach .\n",
      "\n",
      "Inp: расстояние до аэропорта биллунна составляет 55 км .\n",
      "Out: this holiday home is 55 km from billund airport .\n",
      "\n",
      "Inp: неподалеку проходит автомагистраль a73 , по которой за 10 минут можно доехать до города бамберга .\n",
      "Out: the property is just a 10 - minute drive from the property .\n",
      "\n",
      "Inp: расстояние от гостевого дома до музея кисуцкого поселка и исторической железной дороги в вихиловку составляет 15 км , до города жилина – 25 км .\n",
      "Out: the ancient theatre of edison is within 15 km , while the unesco - listed museum of the city of arts , 15 km from the property , while the city of sciences is 25 km away .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::200], attentive_model.translate_lines(dev_inp[::200], device)[0]):\n",
    "    print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inp: в распоряжении гостей караоке .\n",
      "Out: guests can be enjoyed at the property .\n",
      "\n",
      "Inp: гостевой дом hosteria posada drake находится в 800 метрах от автобусного вокзала пуэрто - сан - хулиан .\n",
      "Out: the guest house is located 800 metres from the famous bus station , 800 metres from the nearest bus station .\n",
      "\n",
      "Inp: расстояние до международного аэропорта куала - лумпура составляет 88 км .\n",
      "Out: kuala lumpur international airport is delhi international airport , 88 km from the property .\n",
      "\n",
      "Inp: апартаменты ariosa с бесплатным wi - fi находятся в городе мтвапа .\n",
      "Out: featuring free wifi is located in moscow , apartment features free wifi .\n",
      "\n",
      "Inp: в 500 метрах находится 9 - луночное гольф - поле ehrwald .\n",
      "Out: the property is 500 metres from the property .\n",
      "\n",
      "Inp: к вашим услугам ресторан , где подают блюда из различных уголков мира , а также круглосуточное обслуживание номеров .\n",
      "Out: it has a restaurant , with a selection of chinese and breakfast .\n",
      "\n",
      "Inp: в ресторане über den wolken на 36 этаже отеля можно заказать кофе и пирожные , а из окон открывается вид на залив любек .\n",
      "Out: in the restaurant on the schlöwen ’ s restaurant , lunch and offers views from the mediterranean sea views .\n",
      "\n",
      "Inp: из отеля открывается вид на окружающие горы и пустыню .\n",
      "Out: the hotel boasts views of them and mountain views of the mountains .\n",
      "\n",
      "Inp: прогулка до ближайшей остановки вапоретто giardini - biennale займет 7 минут .\n",
      "Out: the nearest airport is a shopping street from san antonio , 7 minutes ’ walk from the san francisco ’ s shopping area .\n",
      "\n",
      "Inp: апартаменты sunny view north находятся всего в 10 минутах ходьбы от пляжа .\n",
      "Out: located just a 10 - minute walk from the beach , apartments on the sea is just a beach is a 10 - minute walk from the beach .\n",
      "\n",
      "Inp: расстояние до аэропорта биллунна составляет 55 км .\n",
      "Out: this holiday home airport is airport , 55 km from the property .\n",
      "\n",
      "Inp: неподалеку проходит автомагистраль a73 , по которой за 10 минут можно доехать до города бамберга .\n",
      "Out: the property is a 10 - minute walk from the city of the town of the city of the city .\n",
      "\n",
      "Inp: расстояние от гостевого дома до музея кисуцкого поселка и исторической железной дороги в вихиловку составляет 15 км , до города жилина – 25 км .\n",
      "Out: the ancient theatre of economics is 15 - minute drive from the männc and the unesco - protected euphrhauser , is 15 km away .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::200], attentive_model.translate_lines(dev_inp[::200], device, 2)[0]):\n",
    "    print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "    print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentive model gets bleu 24.7, it is significantly better then simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "mname = \"facebook/wmt19-ru-en\"\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "model = FSMTForConditionalGeneration.from_pretrained(mname).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ec7c63e4b5433a9e06efb15afabb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2501.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-9ecc23661e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     print()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 34.61015214984252\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mcompute_blue_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-9ecc23661e48>\u001b[0m in \u001b[0;36mcompute_blue_transformer\u001b[0;34m(model, inp_lines, out_lines, bpe_sep)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, decoder_input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             )\n\u001b[1;32m    491\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_generate_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, num_return_sequences, length_penalty, num_beams, vocab_size, attention_mask, use_cache, model_kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             )\n\u001b[0;32m--> 665\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size * num_beams, cur_len, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (batch_size * num_beams, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_fsmt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, **unused)\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         )\n\u001b[1;32m   1086\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_fsmt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m         )\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_fsmt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, encoder_hidden_states, encoder_padding_mask, decoder_padding_mask, decoder_causal_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, **unused)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0mlayer_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_causal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m             )\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_fsmt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_hidden_states, encoder_attn_mask, layer_state, causal_mask, decoder_padding_mask, output_attentions)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mlayer_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_state\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# mutates layer state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         )\n\u001b[1;32m    552\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/modeling_fsmt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, key_padding_mask, layer_state, attn_mask, output_attentions)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# don't attend to padding symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mreshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def compute_blue_transformer(model, inp_lines, out_lines, bpe_sep='@@ '):\n",
    "    inp = [line.replace(bpe_sep, '') for line in inp_lines]\n",
    "    out = [line.replace(bpe_sep, '') for line in out_lines]\n",
    "    translations = []\n",
    "    for src in tqdm(inp):\n",
    "        input_ids = tokenizer.encode(src, return_tensors=\"pt\").to('cuda:1')\n",
    "        outputs = model.generate(input_ids, num_beams=2)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        decoded = tokenize(decoded)\n",
    "        translations.append(decoded)\n",
    "        \n",
    "    return corpus_bleu(\n",
    "        [[ref.split()] for ref in out],\n",
    "        [trans.split() for trans in translations],\n",
    "        smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
    "        ) * 100\n",
    "    \n",
    "\n",
    "# for inp_line in dev_inp[::500]:\n",
    "#     inp = inp_line.replace('@@ ', '')\n",
    "#     input_ids = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(input_ids)\n",
    "#     decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     decoded = tokenize(decoded)\n",
    "#     print(inp)\n",
    "#     print(decoded)\n",
    "#     print()\n",
    "# 34.61015214984252\n",
    "compute_blue_transformer(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"я не люблю 23 февраля и иду в бгу!\"\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "input_ids\n",
    "# outputs = model.generate(input_ids)\n",
    "# decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(tokenize(decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Attention score was 16, and now is 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFwsvHUOSEid"
   },
   "source": [
    "### Visualizing model attention (2 points)\n",
    "\n",
    "After training the attentive translation model, you can check it's sanity by visualizing its attention weights.\n",
    "\n",
    "We provided you with a function that draws attention maps using [`Bokeh`](https://bokeh.pydata.org/en/latest/index.html). Once you managed to produce something better than random noise, please save at least 3 attention maps and __submit them to anytask__ alongside this notebook to get the max grade. Saving bokeh figures as __cell outputs is not enough!__ (TAs can't see saved bokeh figures in anytask). You can save bokeh images as screenshots or using this button:\n",
    "\n",
    "![bokeh_panel](https://github.com/yandexdataschool/nlp_course/raw/2019/resources/bokeh_panel.png)\n",
    "\n",
    "__Note:__ you're not locked into using bokeh. If you prefer a different visualization method, feel free to use that instead of bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKFgCACwSEid"
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as pl\n",
    "import bokeh.models as bm\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "def draw_attention(inp_line, translation, probs):\n",
    "    \"\"\" An intentionally ambiguous function to visualize attention weights \"\"\"\n",
    "    inp_tokens = inp_voc.tokenize(inp_line)\n",
    "    trans_tokens = out_voc.tokenize(translation)\n",
    "    probs = probs[:len(trans_tokens), :len(inp_tokens)]\n",
    "    \n",
    "    fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),\n",
    "                    x_axis_type=None, y_axis_type=None, tools=[])\n",
    "    fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))\n",
    "\n",
    "    fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')\n",
    "    fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5\n",
    "    fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))\n",
    "    fig.xaxis.major_label_orientation = 45\n",
    "\n",
    "    fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')\n",
    "    fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5\n",
    "    fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))\n",
    "\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLvPbNMzSEig"
   },
   "outputs": [],
   "source": [
    "inp = dev_inp[::500]\n",
    "\n",
    "trans, states = model.translate_lines(inp)\n",
    "\n",
    "# select attention probs from model state (you may need to change this for your custom model)\n",
    "# attention_probs below must have shape [batch_size, translation_length, input_length], extracted from states\n",
    "# e.g. if attention probs are at the end of each state, use np.stack([state[-1] for state in states], axis=1)\n",
    "attention_probs = np.stack([state[-1].detach().cpu().numpy() for state in states], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBBafAOrSEih",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    draw_attention(inp[i], trans[i], attention_probs[i])\n",
    "    \n",
    "# Does it look fine already? don't forget to save images for anytask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Из рисунков видно, что есть Attention, которые смотрят на предыдущие токены. Есть, которые на следующие. Есть, которые еще на что-то интересное. PDF с ними сдам в lms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvByTMaASEik"
   },
   "source": [
    "__Note 1:__ If the attention maps are not iterpretable, try starting encoder from zeros (instead of dec_start), forcing model to use attention.\n",
    "\n",
    "__Note 2:__ If you're studying this course as a YSDA student, please submit __attention screenshots__ alongside your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbIIngNVlrtt"
   },
   "source": [
    "## Goind deeper (2++ points each)\n",
    "\n",
    "We want you to find the best model for the task. Use everything you know.\n",
    "\n",
    "* different recurrent units: rnn/gru/lstm; deeper architectures\n",
    "* bidirectional encoder, different attention methods for decoder (additive, dot-product, multi-head)\n",
    "* word dropout, training schedules, anything you can imagine\n",
    "* replace greedy inference with beam search\n",
    "\n",
    "For a better grasp of seq2seq We recommend you to conduct at least one experiment from one of the bullet-points or your alternative ideas. As usual, describe what you tried and what results you obtained in a short report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А теперь попробуем bidirectional GRU с нашим первым Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size*2, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size+hid_size*2, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "        self.attn = AttentionLayer('kek', hid_size*2, hid_size, hid_size)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        # enc_seq = inp_emb\n",
    "        enc_seq, _ = self.enc0(inp_emb)\n",
    "        # print(enc_seq.shape)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)        \n",
    "        \n",
    "        enc_mask = self.out_voc.compute_mask(inp)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        first_attn = self.attn(enc_seq, dec_start, enc_mask)[1]\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        first_state = [dec_start, enc_seq, enc_mask, first_attn]\n",
    "        return first_state\n",
    "    \n",
    "   \n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        \n",
    "        attn, attn_prob = self.attn(enc_seq, prev_gru0_state, enc_mask)\n",
    "        \n",
    "        x = self.emb_out(prev_tokens)\n",
    "        assert len(x.shape) == 2 and len(attn.shape) == 2\n",
    "        x = torch.cat([attn, x], dim=-1)\n",
    "        x = self.dec0(x, prev_gru0_state)\n",
    "        \n",
    "        new_dec_state = [x, enc_seq, enc_mask, attn_prob]\n",
    "        output_logits = self.logits(x)\n",
    "        \n",
    "        return new_dec_state, output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = AttentiveModel('имя (name)', inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "\n",
    "for _ in trange(25000//2):\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "    loss_t.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    metrics['train_loss'].append((step, loss_t.item()))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "        \n",
    "# Note: it's okay if bleu oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()\n",
    "\n",
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Было blue 20.9, а щас целых 22.32. То есть Bidirectional GRU лучше обычного GRU. Значит с большой вероятностью можно утверждать, что Bid. LSTM и Bid. RNN (обычная RNN) будут лучше своих не Bid. версий. Сначала проверим Bid. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.RNN(emb_size, hid_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size*2, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size+hid_size*2, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "        self.attn = AttentionLayer('kek', hid_size*2, hid_size, hid_size)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        # enc_seq = inp_emb\n",
    "        enc_seq, _ = self.enc0(inp_emb)\n",
    "        # print(enc_seq.shape)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)        \n",
    "        \n",
    "        enc_mask = self.out_voc.compute_mask(inp)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        first_attn = self.attn(enc_seq, dec_start, enc_mask)[1]\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        first_state = [dec_start, enc_seq, enc_mask, first_attn]\n",
    "        return first_state\n",
    "    \n",
    "   \n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        \n",
    "        attn, attn_prob = self.attn(enc_seq, prev_gru0_state, enc_mask)\n",
    "        \n",
    "        x = self.emb_out(prev_tokens)\n",
    "        assert len(x.shape) == 2 and len(attn.shape) == 2\n",
    "        x = torch.cat([attn, x], dim=-1)\n",
    "        x = self.dec0(x, prev_gru0_state)\n",
    "        \n",
    "        new_dec_state = [x, enc_seq, enc_mask, attn_prob]\n",
    "        output_logits = self.logits(x)\n",
    "        \n",
    "        return new_dec_state, output_logits\n",
    "    \n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = AttentiveModel('имя (name)', inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "\n",
    "for _ in trange(25000//2):\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "    loss_t.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    metrics['train_loss'].append((step, loss_t.item()))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "        \n",
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()\n",
    "\n",
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN сработала примерно также как и GRU, странно, ведь GRU - модификация RNN, а сейчас проверим LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=64, hid_size=128, attn_size=128):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.LSTM(emb_size, hid_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size*2, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size+hid_size*2, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "        self.attn = AttentionLayer('kek', hid_size*2, hid_size, hid_size)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        # enc_seq = inp_emb\n",
    "        enc_seq, _ = self.enc0(inp_emb)\n",
    "        # print(enc_seq.shape)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)        \n",
    "        \n",
    "        enc_mask = self.out_voc.compute_mask(inp)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        first_attn = self.attn(enc_seq, dec_start, enc_mask)[1]\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        first_state = [dec_start, enc_seq, enc_mask, first_attn]\n",
    "        return first_state\n",
    "    \n",
    "   \n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        \n",
    "        attn, attn_prob = self.attn(enc_seq, prev_gru0_state, enc_mask)\n",
    "        \n",
    "        x = self.emb_out(prev_tokens)\n",
    "        assert len(x.shape) == 2 and len(attn.shape) == 2\n",
    "        x = torch.cat([attn, x], dim=-1)\n",
    "        x = self.dec0(x, prev_gru0_state)\n",
    "        \n",
    "        new_dec_state = [x, enc_seq, enc_mask, attn_prob]\n",
    "        output_logits = self.logits(x)\n",
    "        \n",
    "        return new_dec_state, output_logits\n",
    "    \n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = AttentiveModel('имя (name)', inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "\n",
    "for _ in trange(25000//2):\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "    loss_t.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    metrics['train_loss'].append((step, loss_t.item()))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "        \n",
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()\n",
    "\n",
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стало еще хуже что-то. Как так? Ведь LSTM лучше RNN должно быть - вот и я не знаю. \n",
    "\n",
    "# Давайте к нашему лучшему варианту (Bid. GRU - дает 22.32 bleu) чуть подберем константы hid_size, emb_size и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class AttentiveModel(BasicModel):\n",
    "    def __init__(self, name, inp_voc, out_voc,\n",
    "                 emb_size=300, hid_size=256, attn_size=256):\n",
    "        \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "        nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
    "        self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "        self.hid_size = hid_size\n",
    "        \n",
    "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dec_start = nn.Linear(hid_size*2, hid_size)\n",
    "        self.dec0 = nn.GRUCell(emb_size+hid_size*2, hid_size)\n",
    "        self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "        self.attn = AttentionLayer('kek', hid_size*2, hid_size, hid_size)\n",
    "\n",
    "    def encode(self, inp, **flags):\n",
    "        \"\"\"\n",
    "        Takes symbolic input sequence, computes initial state\n",
    "        :param inp: matrix of input tokens [batch, time]\n",
    "        :return: a list of initial decoder state tensors\n",
    "        \"\"\"\n",
    "        \n",
    "        inp_emb = self.emb_inp(inp)\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        # enc_seq = inp_emb\n",
    "        enc_seq, _ = self.enc0(inp_emb)\n",
    "        # print(enc_seq.shape)\n",
    "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "        # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "        dec_start = self.dec_start(last_state)        \n",
    "        \n",
    "        enc_mask = self.out_voc.compute_mask(inp)\n",
    "        \n",
    "        # apply attention layer from initial decoder hidden state\n",
    "        first_attn = self.attn(enc_seq, dec_start, enc_mask)[1]\n",
    "        \n",
    "        # Build first state: include\n",
    "        # * initial states for decoder recurrent layers\n",
    "        # * encoder sequence and encoder attn mask (for attention)\n",
    "        # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "        first_state = [dec_start, enc_seq, enc_mask, first_attn]\n",
    "        return first_state\n",
    "    \n",
    "   \n",
    "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "        \"\"\"\n",
    "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "        :param prev_state: a list of previous decoder state tensors\n",
    "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "        :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        \n",
    "        attn, attn_prob = self.attn(enc_seq, prev_gru0_state, enc_mask)\n",
    "        \n",
    "        x = self.emb_out(prev_tokens)\n",
    "        assert len(x.shape) == 2 and len(attn.shape) == 2\n",
    "        x = torch.cat([attn, x], dim=-1)\n",
    "        x = self.dec0(x, prev_gru0_state)\n",
    "        \n",
    "        new_dec_state = [x, enc_seq, enc_mask, attn_prob]\n",
    "        output_logits = self.logits(x)\n",
    "        \n",
    "        return new_dec_state, output_logits\n",
    "\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = AttentiveModel('имя (name)', inp_voc, out_voc).to('cuda:1')\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "\n",
    "for _ in trange(4200):\n",
    "    step = len(metrics['train_loss']) + 1\n",
    "    batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "    batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to('cuda:1')\n",
    "    batch_out = out_voc.to_matrix(train_out[batch_ix]).to('cuda:1')\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "    loss_t.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    metrics['train_loss'].append((step, loss_t.item()))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "            plt.subplot(1, len(metrics), i + 1)\n",
    "            plt.title(name)\n",
    "            plt.plot(*zip(*history))\n",
    "            plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "        \n",
    "for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "    print(inp_line)\n",
    "    print(trans_line)\n",
    "    print()\n",
    "\n",
    "compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24.8 - уже совсем много, думаю на этом можно закончить"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "edk_oVg0lrtW"
   ],
   "name": "practice_and_homework_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
