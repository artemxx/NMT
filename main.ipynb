{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmsFABwClrsS"
   },
   "source": [
    "# Encoder-decoder architecture\n",
    "\n",
    "Encoder-decoder architectures are about converting anything to anything, including\n",
    " * Machine translation and spoken dialogue systems\n",
    " * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://openai.com/requests-for-research/#im2latex) (convolutional encoder, recurrent decoder)\n",
    " * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n",
    " * Grapheme2phoneme - convert words to transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vocab import Vocab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_models import BasicModel\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4N9AD2dlrsU"
   },
   "source": [
    "# Our task: machine translation\n",
    "\n",
    "We gonna try our encoder-decoder models on russian to english machine translation problem. More specifically, we'll translate hotel and hostel descriptions.\n",
    "\n",
    "* Data will be tokenized with WordPunctTokenizer.\n",
    "\n",
    "* Our data lines contain unique rare words. If we operate on a word level, we will have to deal with large vocabulary size. If instead we use character-level models, it would take lots of iterations to process a sequence. This time we're gonna pick something inbetween.\n",
    "\n",
    "* One popular approach is called [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt) aka __BPE__. The algorithm starts with a character-level tokenization and then iteratively merges most frequent pairs for N iterations. This results in frequent words being merged into a single token and rare words split into syllables or even characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "CfvojjHQlrsU",
    "outputId": "aa95011d-c2f7-4ca3-aec1-b246b9c3f886",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch>=1.3.0\n",
    "!pip3 install subword-nmt &> log\n",
    "# !wget https://www.dropbox.com/s/yy2zqh34dyhv07i/data.txt?dl=1 -O data.txt\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/week04_seq2seq/vocab.py -O vocab.py\n",
    "# thanks to tilda and deephack teams for the data, Dmitry Emelyanenko for the code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9kP0SdxlrsY"
   },
   "outputs": [],
   "source": [
    "# def tokenize(x):\n",
    "#     return ' '.join(nltk_tokenizer.tokenize(x.lower()))\n",
    "\n",
    "\n",
    "# nltk_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# # split and tokenize the data\n",
    "# with open('train.en', 'w') as f_src,  open('train.ru', 'w') as f_dst:\n",
    "#     for line in open('data.txt'):\n",
    "#         src_line, dst_line = line.strip().split('\\t')\n",
    "#         f_src.write(tokenize(src_line) + '\\n')\n",
    "#         f_dst.write(tokenize(dst_line) + '\\n')\n",
    "\n",
    "# # build and apply bpe vocs\n",
    "# bpe = {}\n",
    "# for lang in ['en', 'ru']:\n",
    "#     learn_bpe(open('./train.' + lang), open('bpe_rules.' + lang, 'w'), num_symbols=8000)\n",
    "#     bpe[lang] = BPE(open('./bpe_rules.' + lang))\n",
    "    \n",
    "#     with open('train.bpe.' + lang, 'w') as f_out:\n",
    "#         for line in open('train.' + lang):\n",
    "#             f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UPW3sV8lrsb"
   },
   "source": [
    "## Building vocabularies\n",
    "\n",
    "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "8PskgBSxlrsd",
    "outputId": "dfb78818-90cf-4138-c1f6-3f492ba06a3c"
   },
   "outputs": [],
   "source": [
    "data_inp = np.array(open('./train.bpe.ru').read().split('\\n'))\n",
    "data_out = np.array(open('./train.bpe.en').read().split('\\n'))\n",
    "\n",
    "assert len(data_inp) == len(data_out), 'Number of inp & out sequences must be the same'\n",
    "\n",
    "print('Number of sequences: %s' % len(data_inp), end='\\n\\n')\n",
    "\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=0.05,\n",
    "                                                          random_state=0)\n",
    "for i in range(3):\n",
    "    print('Inp:', train_inp[i])\n",
    "    print('Out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vipg4O61lrsg"
   },
   "outputs": [],
   "source": [
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "cwOoHfuhlrsi",
    "outputId": "3a94bd2b-f34c-49ce-ad13-1ee1215d76c4"
   },
   "outputs": [],
   "source": [
    "# Here's how you cast lines into ids and backwards\n",
    "batch_lines = sorted(train_inp, key=len)[5:10]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print('Lines: %s' % batch_lines, end='\\n\\n')\n",
    "print('Words to ids (0 = BOS, 1 = EOS):\\n %s' % batch_ids, end='\\n\\n') # BOS = Begin Of Sentence\n",
    "print('Back to words: %s' % batch_lines_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSYu-MkElrsk"
   },
   "source": [
    "## Draw source and translation length distributions to estimate the scope of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "TLLl9cSNlrsl",
    "outputId": "fa1ad0ae-c0b1-4199-f2c6-5dd16d8aa303"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"source length\")\n",
    "plt.hist(list(map(len, map(str.split, train_inp))), bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"translation length\")\n",
    "plt.hist(list(map(len, map(str.split, train_out))), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHWgx34flrsn"
   },
   "source": [
    "## Encoder-decoder model\n",
    "\n",
    "The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder, no attention or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pd_rDRm9lrso",
    "outputId": "65302dbe-4309-412d-f460-3c4f87054236"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzdVCCwOSEhv"
   },
   "outputs": [],
   "source": [
    "model = BasicModel(inp_voc, out_voc).to(device)\n",
    "\n",
    "dummy_inp_tokens = inp_voc.to_matrix(sorted(train_inp, key=len)[5:10]).to(device)\n",
    "dummy_out_tokens = out_voc.to_matrix(sorted(train_out, key=len)[5:10]).to(device)\n",
    "\n",
    "h0 = model.encode(dummy_inp_tokens)\n",
    "h1, logits1 = model.decode_step(h0, torch.arange(len(dummy_inp_tokens), device=device))\n",
    "\n",
    "assert isinstance(h1, list) and len(h1) == len(h0)\n",
    "assert h1[0].shape == h0[0].shape and not torch.allclose(h1[0], h0[0])\n",
    "assert logits1.shape == (len(dummy_inp_tokens), len(out_voc))\n",
    "\n",
    "logits_seq = model.decode(h0, dummy_out_tokens)\n",
    "assert logits_seq.shape == (dummy_out_tokens.shape[0], dummy_out_tokens.shape[1], len(out_voc))\n",
    "\n",
    "# full forward\n",
    "logits_seq2 = model(dummy_inp_tokens, dummy_out_tokens)\n",
    "assert logits_seq2.shape == logits_seq.shape\n",
    "\n",
    "dummy_translations, dummy_states = model.translate_lines(train_inp[:2], device, max_len=25)\n",
    "translations = '\\n'.join([line for line in dummy_translations])\n",
    "print('Translations without training:\\n%s' % translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wuv1-aVlrs0"
   },
   "source": [
    "## Training loss\n",
    "\n",
    "Our training objective is almost the same as it was for neural language models:\n",
    "$$ L = {\\frac1{|D|}} \\sum_{X, Y \\in D} \\sum_{y_t \\in Y} - \\log p(y_t \\mid y_1, \\dots, y_{t-1}, X, \\theta) $$\n",
    "\n",
    "where $|D|$ is the __total length of all sequences__, including BOS and first EOS, but excluding PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commons import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8XPV8sWlrs5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_loss = compute_loss(model, dummy_inp_tokens, dummy_out_tokens)\n",
    "print('Loss: %s' % dummy_loss)\n",
    "assert np.allclose(dummy_loss.item(), 7.5, rtol=0.1, atol=0.1), 'Sorry for your loss'\n",
    "\n",
    "# test autograd\n",
    "dummy_loss.backward()\n",
    "for name, param in model.named_parameters():\n",
    "    assert param.grad is not None and abs(param.grad.max()) != 0, 'Param %s received no gradients' % name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpbaBpW7lrs-"
   },
   "source": [
    "## Evaluation: BLEU\n",
    "\n",
    "Machine translation is commonly evaluated with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. This metric simply computes which fraction of predicted n-grams is actually present in the reference translation. It does so for n=1,2,3 and 4 and computes the geometric average with penalty if translation is shorter than reference.\n",
    "\n",
    "While BLEU [has many drawbacks](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf), it still remains the most commonly used metric and one of the simplest to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commons import compute_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb1-PhKIlrs-"
   },
   "outputs": [],
   "source": [
    "# compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQDhGwg4lrtC"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfwIaixHlrtI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, n_iters=15000, batch_size=64, lr=1e-3, n_iters_break=500):\n",
    "    metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_weights = model.state_dict()\n",
    "    best_bleu = -1\n",
    "    best_step = -1\n",
    "    with tqdm(range(n_iters)) as progress_bar:\n",
    "        for i in progress_bar:\n",
    "            step = len(metrics['train_loss']) + 1\n",
    "            batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "            batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to(device)\n",
    "            batch_out = out_voc.to_matrix(train_out[batch_ix]).to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "            loss_t.backward()\n",
    "            opt.step()\n",
    "\n",
    "            metrics['train_loss'].append((step, loss_t.item()))\n",
    "            if step % 100 == 0:\n",
    "                bleu = compute_bleu(model, dev_inp, dev_out)\n",
    "                metrics['dev_bleu'].append((step, bleu))\n",
    "\n",
    "                if bleu > best_bleu:\n",
    "                    best_bleu = bleu\n",
    "                    best_step = step\n",
    "                    best_weights = model.state_dict()\n",
    "                elif step - best_step >= n_iters_break:\n",
    "                    break\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                plt.figure(figsize=(12,4))\n",
    "                for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                    plt.subplot(1, len(metrics), i + 1)\n",
    "                    plt.title(name)\n",
    "                    plt.plot(*zip(*history))\n",
    "                    plt.grid()\n",
    "                plt.show()\n",
    "\n",
    "                print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "            \n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "LlDT6eDUlrtL",
    "outputId": "d0299ee4-2afb-466b-b767-98c09d6ee5e3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic_model = BasicModel(inp_voc, out_voc).to(device)\n",
    "# %time train_model(basic_model, batch_size=128, lr=1e-3*5, n_iters=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(basic_model.state_dict(), \"basic_model\")\n",
    "# basic_model.load_state_dict(torch.load(\"basic_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def beam_research(model, inp, out, model_name, beam_min=1, beam_max=10):\n",
    "#     beam_range = range(beam_min, beam_max + 1)\n",
    "#     blue_scores = []\n",
    "\n",
    "#     for beam_size in beam_range:\n",
    "#         blue = compute_bleu(model, inp, out, beam_size=beam_size)\n",
    "#         blue_scores.append(blue)\n",
    "\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     plt.plot(beam_range, blue_scores)\n",
    "#     plt.title('Beam search results for %s' % model_name)\n",
    "#     plt.xticks(beam_range)\n",
    "#     plt.xlabel('beam size')\n",
    "#     plt.ylabel('blue score')\n",
    "#     plt.show()\n",
    "\n",
    "# %time beam_research(basic_model, dev_inp, dev_out, 'basic model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyaHOpealrtS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for inp_line, trans_line in zip(dev_inp[::700], basic_model.translate_lines(dev_inp[::700], device, 2)[0]):\n",
    "#         print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "#         print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Encoder-decoder model gets bleu 17.5, but quality of translation is so bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz9aROAIlrtX"
   },
   "source": [
    "### Attention layer\n",
    "\n",
    "Here you will have to implement a layer that computes a simple additive attention:\n",
    "\n",
    "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
    "\n",
    "* Compute logits with a 2-layer neural network\n",
    "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
    "* Get probabilities from logits, \n",
    "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
    "\n",
    "* Add up encoder states with probabilities to get __attention response__\n",
    "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
    "\n",
    "You can learn more about attention layers in the lecture slides or [from this post](https://distill.pub/2016/augmented-rnns/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IalfpdAelrtb"
   },
   "source": [
    "## Seq2seq model with attention\n",
    "\n",
    "You can now use the attention layer to build a network. The simplest way to implement attention is to use it in decoder phase. On every step, use __previous__ decoder state to obtain attention response. Then feed concat this response to the inputs of next attention layer.\n",
    "\n",
    "The key implementation detail here is __model state__. Put simply, you can add any tensor into the list of `encode` outputs. You will then have access to them at each `decode` step. This may include:\n",
    "* Last RNN hidden states (as in basic model)\n",
    "* The whole sequence of encoder outputs (to attend to) and mask\n",
    "* Attention probabilities (to visualize)\n",
    "\n",
    "_There are alternative ways to wire attention into network and different kinds of attention. For example [this](https://arxiv.org/abs/1609.08144), [this](https://arxiv.org/abs/1706.03762) and [this](https://arxiv.org/abs/1808.03867) for ideas. And for image captioning/im2latex there's [visual attention](https://arxiv.org/abs/1502.03044)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCKPB5JmcE6j"
   },
   "outputs": [],
   "source": [
    "from my_models import AttentiveModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryZCOTEslrtf"
   },
   "source": [
    "### Training attentive model\n",
    "\n",
    "Please reuse the infrastructure you've built for the regular model. I hope you didn't hard-code anything :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YMHPgZxcFaQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# attentive_model = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "# %time train_model(attentive_model, n_iters=5000, lr=1e-3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(attentive_model.state_dict(), \"att_model\")\n",
    "# attentive_model.load_state_dict(torch.load(\"att_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time beam_research(attentive_model, dev_inp, dev_out, 'attentive model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for inp_line, trans_line in zip(dev_inp[::700], attentive_model.translate_lines(dev_inp[::700], device, 4)[0]):\n",
    "#         print('Inp: %s' % inp_line.replace('@@ ', ''))\n",
    "#         print('Out: %s' % trans_line.replace('@@ ', ''))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentive model gets bleu 26.2, it is significantly better then simple model and translation so better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run my_models.py\n",
    "scores = []\n",
    "for bid in [True, False]:\n",
    "    for rnn_type in ['LSTM', 'GRU', 'RNN']:\n",
    "        model = AttentiveModel(inp_voc, out_voc, rnn_type=rnn_type, bid=bid).to(device)\n",
    "        bleu = train_model(model, lr=1e-3*2, batch_size=10)\n",
    "        scores.append(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mname = \"facebook/wmt19-ru-en\"\n",
    "# tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "# model = FSMTForConditionalGeneration.from_pretrained(mname).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def compute_blue_transformer(model, inp_lines, out_lines, bpe_sep='@@ '):\n",
    "#     inp = [line.replace(bpe_sep, '') for line in inp_lines]\n",
    "#     out = [line.replace(bpe_sep, '') for line in out_lines]\n",
    "#     translations = []\n",
    "#     for src in tqdm(inp):\n",
    "#         input_ids = tokenizer.encode(src, return_tensors=\"pt\").to('cuda:1')\n",
    "#         outputs = model.generate(input_ids, num_beams=2)\n",
    "#         decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#         decoded = tokenize(decoded)\n",
    "#         translations.append(decoded)\n",
    "        \n",
    "#     return corpus_bleu(\n",
    "#         [[ref.split()] for ref in out],\n",
    "#         [trans.split() for trans in translations],\n",
    "#         smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
    "#         ) * 100\n",
    "    \n",
    "\n",
    "# # for inp_line in dev_inp[::500]:\n",
    "# #     inp = inp_line.replace('@@ ', '')\n",
    "# #     input_ids = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "# #     outputs = model.generate(input_ids)\n",
    "# #     decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# #     decoded = tokenize(decoded)\n",
    "# #     print(inp)\n",
    "# #     print(decoded)\n",
    "# #     print()\n",
    "# compute_blue_transformer(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"я не люблю 23 февраля и иду в бгу!\"\n",
    "# input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "# outputs = model.generate(input_ids)\n",
    "# decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(tokenize(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Attention score was 16, and now is 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFwsvHUOSEid"
   },
   "source": [
    "### Visualizing model attention (2 points)\n",
    "\n",
    "After training the attentive translation model, you can check it's sanity by visualizing its attention weights.\n",
    "\n",
    "We provided you with a function that draws attention maps using [`Bokeh`](https://bokeh.pydata.org/en/latest/index.html). Once you managed to produce something better than random noise, please save at least 3 attention maps and __submit them to anytask__ alongside this notebook to get the max grade. Saving bokeh figures as __cell outputs is not enough!__ (TAs can't see saved bokeh figures in anytask). You can save bokeh images as screenshots or using this button:\n",
    "\n",
    "![bokeh_panel](https://github.com/yandexdataschool/nlp_course/raw/2019/resources/bokeh_panel.png)\n",
    "\n",
    "__Note:__ you're not locked into using bokeh. If you prefer a different visualization method, feel free to use that instead of bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKFgCACwSEid"
   },
   "outputs": [],
   "source": [
    "# import bokeh.plotting as pl\n",
    "# import bokeh.models as bm\n",
    "# from bokeh.io import output_notebook, show\n",
    "# output_notebook()\n",
    "\n",
    "# def draw_attention(inp_line, translation, probs):\n",
    "#     \"\"\" An intentionally ambiguous function to visualize attention weights \"\"\"\n",
    "#     inp_tokens = inp_voc.tokenize(inp_line)\n",
    "#     trans_tokens = out_voc.tokenize(translation)\n",
    "#     probs = probs[:len(trans_tokens), :len(inp_tokens)]\n",
    "    \n",
    "#     fig = pl.figure(x_range=(0, len(inp_tokens)), y_range=(0, len(trans_tokens)),\n",
    "#                     x_axis_type=None, y_axis_type=None, tools=[])\n",
    "#     fig.image([probs[::-1]], 0, 0, len(inp_tokens), len(trans_tokens))\n",
    "\n",
    "#     fig.add_layout(bm.LinearAxis(axis_label='source tokens'), 'above')\n",
    "#     fig.xaxis.ticker = np.arange(len(inp_tokens)) + 0.5\n",
    "#     fig.xaxis.major_label_overrides = dict(zip(np.arange(len(inp_tokens)) + 0.5, inp_tokens))\n",
    "#     fig.xaxis.major_label_orientation = 45\n",
    "\n",
    "#     fig.add_layout(bm.LinearAxis(axis_label='translation tokens'), 'left')\n",
    "#     fig.yaxis.ticker = np.arange(len(trans_tokens)) + 0.5\n",
    "#     fig.yaxis.major_label_overrides = dict(zip(np.arange(len(trans_tokens)) + 0.5, trans_tokens[::-1]))\n",
    "\n",
    "#     show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLvPbNMzSEig"
   },
   "outputs": [],
   "source": [
    "# inp = dev_inp[::500]\n",
    "\n",
    "# trans, states = model.translate_lines(inp)\n",
    "\n",
    "# # select attention probs from model state (you may need to change this for your custom model)\n",
    "# # attention_probs below must have shape [batch_size, translation_length, input_length], extracted from states\n",
    "# # e.g. if attention probs are at the end of each state, use np.stack([state[-1] for state in states], axis=1)\n",
    "# attention_probs = np.stack([state[-1].detach().cpu().numpy() for state in states], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBBafAOrSEih",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     draw_attention(inp[i], trans[i], attention_probs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Из рисунков видно, что есть Attention, которые смотрят на предыдущие токены. Есть, которые на следующие. Есть, которые еще на что-то интересное. PDF с ними сдам в lms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvByTMaASEik"
   },
   "source": [
    "__Note 1:__ If the attention maps are not iterpretable, try starting encoder from zeros (instead of dec_start), forcing model to use attention.\n",
    "\n",
    "__Note 2:__ If you're studying this course as a YSDA student, please submit __attention screenshots__ alongside your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbIIngNVlrtt"
   },
   "source": [
    "## Goind deeper (2++ points each)\n",
    "\n",
    "We want you to find the best model for the task. Use everything you know.\n",
    "\n",
    "* different recurrent units: rnn/gru/lstm; deeper architectures\n",
    "* bidirectional encoder, different attention methods for decoder (additive, dot-product, multi-head)\n",
    "* word dropout, training schedules, anything you can imagine\n",
    "* replace greedy inference with beam search\n",
    "\n",
    "For a better grasp of seq2seq We recommend you to conduct at least one experiment from one of the bullet-points or your alternative ideas. As usual, describe what you tried and what results you obtained in a short report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А теперь попробуем bidirectional GRU с нашим первым Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Было blue 20.9, а щас целых 22.32. То есть Bidirectional GRU лучше обычного GRU. Значит с большой вероятностью можно утверждать, что Bid. LSTM и Bid. RNN (обычная RNN) будут лучше своих не Bid. версий. Сначала проверим Bid. RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN сработала примерно также как и GRU, странно, ведь GRU - модификация RNN, а сейчас проверим LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стало еще хуже что-то. Как так? Ведь LSTM лучше RNN должно быть - вот и я не знаю. \n",
    "\n",
    "# Давайте к нашему лучшему варианту (Bid. GRU - дает 22.32 bleu) чуть подберем константы hid_size, emb_size и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class AttentiveModel(BasicModel):\n",
    "#     def __init__(self, name, inp_voc, out_voc,\n",
    "#                  emb_size=300, hid_size=256, attn_size=256):\n",
    "#         \"\"\" Translation model that uses attention. See instructions above. \"\"\"\n",
    "#         nn.Module.__init__(self)  # initialize base class to track sub-layers, trainable variables, etc.\n",
    "#         self.inp_voc, self.out_voc = inp_voc, out_voc\n",
    "#         self.hid_size = hid_size\n",
    "        \n",
    "#         self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
    "#         self.emb_out = nn.Embedding(len(out_voc), emb_size)\n",
    "#         self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "#         self.dec_start = nn.Linear(hid_size*2, hid_size)\n",
    "#         self.dec0 = nn.GRUCell(emb_size+hid_size*2, hid_size)\n",
    "#         self.logits = nn.Linear(hid_size, len(out_voc))\n",
    "        \n",
    "#         self.attn = AttentionLayer('kek', hid_size*2, hid_size, hid_size)\n",
    "\n",
    "#     def encode(self, inp, **flags):\n",
    "#         \"\"\"\n",
    "#         Takes symbolic input sequence, computes initial state\n",
    "#         :param inp: matrix of input tokens [batch, time]\n",
    "#         :return: a list of initial decoder state tensors\n",
    "#         \"\"\"\n",
    "        \n",
    "#         inp_emb = self.emb_inp(inp)\n",
    "#         batch_size = inp.shape[0]\n",
    "        \n",
    "#         # enc_seq = inp_emb\n",
    "#         enc_seq, _ = self.enc0(inp_emb)\n",
    "#         # print(enc_seq.shape)\n",
    "#         # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
    "        \n",
    "#         # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
    "#         lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
    "#         last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
    "#         # ^-- shape: [batch_size, hid_size]\n",
    "        \n",
    "#         dec_start = self.dec_start(last_state)        \n",
    "        \n",
    "#         enc_mask = self.out_voc.compute_mask(inp)\n",
    "        \n",
    "#         # apply attention layer from initial decoder hidden state\n",
    "#         first_attn = self.attn(enc_seq, dec_start, enc_mask)[1]\n",
    "        \n",
    "#         # Build first state: include\n",
    "#         # * initial states for decoder recurrent layers\n",
    "#         # * encoder sequence and encoder attn mask (for attention)\n",
    "#         # * make sure that last state item is attention probabilities tensor\n",
    "        \n",
    "#         first_state = [dec_start, enc_seq, enc_mask, first_attn]\n",
    "#         return first_state\n",
    "    \n",
    "   \n",
    "#     def decode_step(self, prev_state, prev_tokens, **flags):\n",
    "#         \"\"\"\n",
    "#         Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
    "#         :param prev_state: a list of previous decoder state tensors\n",
    "#         :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
    "#         :return: a list of next decoder state tensors, a tensor of logits [batch, n_tokens]\n",
    "#         \"\"\"\n",
    "        \n",
    "#         prev_gru0_state, enc_seq, enc_mask, _ = prev_state\n",
    "        \n",
    "#         attn, attn_prob = self.attn(enc_seq, prev_gru0_state, enc_mask)\n",
    "        \n",
    "#         x = self.emb_out(prev_tokens)\n",
    "#         assert len(x.shape) == 2 and len(attn.shape) == 2\n",
    "#         x = torch.cat([attn, x], dim=-1)\n",
    "#         x = self.dec0(x, prev_gru0_state)\n",
    "        \n",
    "#         new_dec_state = [x, enc_seq, enc_mask, attn_prob]\n",
    "#         output_logits = self.logits(x)\n",
    "        \n",
    "#         return new_dec_state, output_logits\n",
    "\n",
    "# metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "# model = AttentiveModel('имя (name)', inp_voc, out_voc).to('cuda:1')\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# batch_size = 64\n",
    "\n",
    "# for _ in trange(4200):\n",
    "#     step = len(metrics['train_loss']) + 1\n",
    "#     batch_ix = np.random.randint(len(train_inp), size=batch_size)\n",
    "#     batch_inp = inp_voc.to_matrix(train_inp[batch_ix]).to('cuda:1')\n",
    "#     batch_out = out_voc.to_matrix(train_out[batch_ix]).to('cuda:1')\n",
    "    \n",
    "#     opt.zero_grad()\n",
    "#     loss_t = compute_loss(model, batch_inp, batch_out)\n",
    "#     loss_t.backward()\n",
    "#     opt.step()\n",
    "    \n",
    "#     metrics['train_loss'].append((step, loss_t.item()))\n",
    "    \n",
    "#     if step % 100 == 0:\n",
    "#         metrics['dev_bleu'].append((step, compute_bleu(model, dev_inp, dev_out)))\n",
    "        \n",
    "#         clear_output(True)\n",
    "#         plt.figure(figsize=(12,4))\n",
    "#         for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "#             plt.subplot(1, len(metrics), i + 1)\n",
    "#             plt.title(name)\n",
    "#             plt.plot(*zip(*history))\n",
    "#             plt.grid()\n",
    "#         plt.show()\n",
    "#         print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)\n",
    "        \n",
    "# for inp_line, trans_line in zip(dev_inp[::500], model.translate_lines(dev_inp[::500])[0]):\n",
    "#     print(inp_line)\n",
    "#     print(trans_line)\n",
    "#     print()\n",
    "\n",
    "# compute_bleu(model, dev_inp, dev_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24.8 - уже совсем много, думаю на этом можно закончить"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "edk_oVg0lrtW"
   ],
   "name": "practice_and_homework_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
